{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search and Cross-Validation\n",
    "\n",
    "_Author: Michael Frantz (LA)_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip --quiet install mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "from sklearn.linear_model import Ridge, Lasso, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: Bias and Variance\n",
    "\n",
    "---\n",
    "![](http://tomrobertshaw.net/img/2015/12/overfitting.jpg)\n",
    "\n",
    "### Discussion:\n",
    "Answer these three questions about the above models given the data. \n",
    "\n",
    "1. Out of these three, which one would you pick as a low bias model? Why?\n",
    "1. Out of these three, which would you pick as a low variance model? Why?\n",
    "1. Out of these three, which one would you chose? Why?\n",
    "1. What are some current methods we have learned for **reducing variance**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters\n",
    "---\n",
    "\n",
    "We've now covered a number of regressors and classifiers, including:\n",
    "* `KNeighborsClassifier`/`KNeighborRegressor`\n",
    "* `LogisticRegression`\n",
    "* `Ridge`\n",
    "* `Lasso`\n",
    "* `SVC`/`SVR`\n",
    "* `DecisionTreeClassifier`/`DecisionTreeRegressor`\n",
    "\n",
    "Each of these models have what are called **hyperparameters**, which affect how the model learns the data. Think of them like dials, and by changing them, we can optimize a model's performance on new data (a.k.a. **reduce variance**)!\n",
    "\n",
    "* What is the hyperparameter for `KNeighborsClassifier`?\n",
    "* What are the hyperparameters for `Ridge`?\n",
    "\n",
    "Up until now, we've used:\n",
    "> **\"Training set\":** the subset of the data that we fit our model on.\n",
    "\n",
    "> **\"Testing set\":** the subset of the data that we evaluate the quality of our predictions on.\n",
    "\n",
    "We've tuned our hyperparameters by hand so our model performs well on a test set. But now, instead of overfitting to our training set, we may be overfitting to our test set! Ultimately, we want to find optimal values for our hyperparameters **without touching our test data**. **BUT HOW?!?!?!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trick is to add *another* set of splits. We're going to use cross-validation **within the training set** to fit our models and tune our hyperparamets, then **evaluate** on the test set. Let's spend a minute on the figure below. Notice:\n",
    "\n",
    "* When we originally fit our models, we use cross-validation to identify bias and variance, and adjust our hyperamaters to improve our model.\n",
    "* In the top panel, we don't do anything with our test set.\n",
    "* In the bottom panel, once we've tuned our hyperparameters, we can **re-fit** on all our training data to predict our **test** data and perform a model evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plot_improper_preprocessing.plot_improper_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing (drum roll please...) `GridSearchCV`\n",
    "\n",
    "What if there was a way we could reduce variance by tuning hyperparameters without even touching our test dataset? To do this, we use the `sklearn` class `GridSearchCV`. Let's dive in:\n",
    "\n",
    "### Setting up `GridSearchCV`\n",
    "\n",
    "##### 1. Estimator\n",
    "\n",
    "First, pick your estimator. This can be any `sklearn` model. Today, we'll use `KNeighborsClassifier`. This is passed to the `estimator` argument.\n",
    "\n",
    "##### 2. Parameter Grid:\n",
    "\n",
    "The second thing you should do when setting up a grid search is create a parameter grid. This allows you to input values for all the hyperparameters you'd like to tune! Here's an example of a parameter grid for `KNeighborsClassifier`. The only hyperparameter we can tune here is `n_neighbors`, which makes this task pretty simple. Let's try every value between 1 and 10.\n",
    "\n",
    "```\n",
    "knc_params = {\n",
    "        'n_neighbors':range(1,11)\n",
    "    }\n",
    "```\n",
    "Your param grid is then passed to your grid search object through the argument `param_grid`.\n",
    "\n",
    "##### 3.  Cross-Valisation:\n",
    "\n",
    "Cross-validation is defined by the `cv=` argument. `cv` can be an integer, `ShuffleSplit`, or `StratifiedShuffleSplit` object.\n",
    "* `cv=5` will perform 5-fold cross-validation\n",
    "* `ShuffleSplit` will shuffle your samples for random validation sets. Default `cv` argument for this object is 10. If you want 5-fold cross-validation using all the data with `ShuffleSplit`, use `cv=ShuffleSplit(cv=10, test_size = .2)`.\n",
    "* `StratifiedShuffleSplit` is similar to `ShuffleSplit`, but can only be used for classificaion. This ensures that the class proportion in your target are preserved in each validation split.\n",
    "\n",
    "\n",
    "##### Note:\n",
    "\n",
    "`GridSearchCV` also takes an argument for `n_jobs`, with a default of 1. Many `sklearn` objects take this argument. If you set `n_jobs=-1`, `sklearn` will parallelize this process accross all the cores in your machine. This can really help speed up model training!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now what does `GridSearchCV` do with that info?\n",
    "\n",
    "When we use `.fit()` on your instantiated `GridSearchCV` object, it does a few things:\n",
    "1. For each combination of hyperparameters, it performs cross-validation and gets a mean train score and mean test score. **Question:** If we do 5-fold cross validaion on a `param_grid` for `Ridge` where `param_grid = {'c':range(1,6)}`, how many individual models are fit?\n",
    "2. For the combination of parameters with the **best mean test score**, `GridSearchCV` re-fits **all the training data** using those best parameters. \n",
    "\n",
    "**Question:** If your `GridSearchCV` is taking *REEEEEEEAAALLY* long to run `.fit()`, what are some options you have to reduce the time?\n",
    "\n",
    "Let's spend a minute and review the graphic below to review what `GridSearchCV` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plot_grid_search.plot_grid_search_overview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, enough talk! Let's code it up. \n",
    "\n",
    "**NOTE:** Because we want to spend time with GridSearch, I've left the data cleaning in for you. However, if you have time, it would be great to review the steps taken to clean this data and try replicating them on your own. I've explained what we're doing step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit, StratifiedShuffleSplit, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datasets/basketball_data.csv')\n",
    "# Set the index to the `GameID`\n",
    "data.set_index('GameId', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the month and day of week from the data using `pandas datetime` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the GameDate colum to datetime type\n",
    "data.GameDate = pd.to_datetime(data['GameDate'])\n",
    "# Extract the day of week and month of the game into their own columns\n",
    "data['GameMonth'] = data['GameDate'].apply(lambda x: x.month)\n",
    "data['GameDayofWeek'] = data['GameDate'].apply(lambda x: x.dayofweek)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the non-numeric columns in this dataset using `df.select_dtypes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['object']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make our target whether the home team wins in a new columna called `host_wins`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['host_wins'] = data['HostName'] == data['winner']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the winner and loser columns from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['winner','loser','GameDate'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have a few `object` columns in this dataset. Let's use `pd.get_dummies` to change these into numeric columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies = pd.get_dummies(data)\n",
    "data_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split the data into our `features` and `target`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data_dummies['host_wins']\n",
    "features = data_dummies.drop('host_wins',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now's where you take over! \n",
    "\n",
    "### Exercise: `GridSearchCV` Workflow\n",
    "\n",
    "Perform a `train_test_split` on the data: \n",
    "* Your split data `X_train, X_test, y_train, y_test`. \n",
    "* Use a `test_size` of .25\n",
    "* Use a `random_state` of 42 so we all have the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `KNeighborsClassifier` and `LogisticRegression`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a `StandardScaler` object. `.fit` the scaler object on your training data (`X_train`), and then `.transform` both your training and test features so they are all scaled.\n",
    "\n",
    "Coming out of this, you should have two new `np.ndarray`s: `X_train_scaled` and `X_test_scaled`. \n",
    "\n",
    "**Question:** Why do we fit the scaler on only the training data? Why do we transform both the train and test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's set up a `param_grid` for KNeighborsClassifier. Let's try neighbors from 5 to 50 with intervals of 5. Ex: `[5,10,15,...,45,50]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there! Lets' instantiate our `GridSearchCV` object where...\n",
    "* our `estimator` is `KNeighborsClassifier`\n",
    "* our `param_grid` is our `knc_params`\n",
    "* our `cv=5` (5-fold cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knc_gs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `.fit` once to fit ALL OUR MODELS!\n",
    "\n",
    "**Question:** Which of the below dataset pairs do we use to fit on? Why?\n",
    "1. `.fit(X_train, y_test)`\n",
    "1. `.fit(X_test, y_train)`\n",
    "1. `.fit(X_train, y_train)`\n",
    "1. `.fit(X_test, y_test)`\n",
    "1. `.fit(X_train, y_train)`\n",
    "1. `.fit(X_train_scaled, y_train)`\n",
    "1. `.fit(X_test_scaled, y_test)`\n",
    "1. `.fit(X_train_scaled, y_train_scaled)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That took way longer than we're used to... \n",
    "**Question:** Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore the **methods** and **attributes** available to our fit `GridSearchCV` object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.cv_results_`\n",
    "\n",
    "This method summarizes all the results of all our cross-validated models. Let's put it and a DataFrame and see what we get!\n",
    "\n",
    "Notice the column names. Since we did 5-fold cross-validation, every time we see a mean, we're evaluating the mean of 5 individual fits on different folds of our cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, let's sort our `cv_results` dataframe by `mean_test_score` using `.sort_values`. \n",
    "\n",
    "**Question:** How many neighbors give us our best score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.best_score_`\n",
    "\n",
    "Let's say we don't want to look at the `cv_results_`. This attribute gives the **best mean test score** from our `cv_results_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.best_params_`\n",
    "\n",
    "This attribute returns the parameters for the model that got the best mean test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.best_estimator_`\n",
    "\n",
    "The `best_estimator_` is an **actual model**. When we call `.predict()` on our `GridSearchCV` object, this is the model that it's using in the background. Notice that the `best_estimator_` has the hyperparameters from `best_params_`. Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.predict()`\n",
    "\n",
    "This works just like the `predict` method on any other `sklearn` estimator. Let's try it out creating a `y_test_pred` array!\n",
    "\n",
    "**Note:** If your `estimator` is a classifier, you also have `.predict_proba()` available to you.\n",
    "\n",
    "**Question:** Which dataset goes into the `.predict()` method if we want our test predictions?\n",
    "\n",
    "1. `X_train`\n",
    "1. `X_train_scaled`\n",
    "1. `X_test`\n",
    "1. `X_test_scaled`\n",
    "1. `y_train`\n",
    "1. `y_train_scaled`\n",
    "1. `y_test`\n",
    "1. `y_test_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `.score()` \n",
    "\n",
    "This **inherits** the `.score()` functoin from our `estimator`. So, if we're doing classification, the default will be accuracy, and for regressors, the default will be $r^2$.\n",
    "\n",
    "What is the **train score**? What is the **test score**? What's your interpretation about our model's capacity to generalize to unseen data? What does that mean in terms of bias and variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `train` and `test` y's, what percent of the time does the home team win? i.e. what is the accuracy if we guessed the home team won every time? (`y_pred` is all ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does our model out-perform chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's go back up to where we define our grid search object and use `StratifiedShuffleSplit` as an example.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (if time permits) Exercise:\n",
    "\n",
    "Use `GridSearchCV` with `LogisticRegression` to see if this model out-performs your nearest neighbors model.\n",
    "\n",
    "* What hyperparameters can we tune for `LogisticRegression`? Let's tune `C` and `penalty`. Let's use a `np.logspace(-3,3,7)` for our `C` hyperparameter, and try both a `l1` and `l2` penalty. \n",
    "* Try using `ShuffleSplit` or `StratifiedShuffleSplit` with 5-fold cross-validation.\n",
    "\n",
    "What are the `best_params_`? The `best_score_`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrgs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot out our mean test score over different values of `C` from our `.cv_results_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_penalty = lrgs.best_params_['penalty']\n",
    "lrgs_results_df = pd.DataFrame(lrgs.cv_results_)\n",
    "\n",
    "(lrgs_results_df[lrgs_results_df['param_penalty'] == best_penalty]\n",
    " [['mean_train_score','mean_test_score','param_C']]\n",
    " .plot(x='param_C'))\n",
    "\n",
    "plt.axvline(lrgs_results_df[lrgs_results_df['param_penalty'] == 'l1']['mean_test_score'].max(), c='r', ls='--', label = 'optimal C')\n",
    "\n",
    "plt.legend()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Conclusion...\n",
    "\n",
    "`GridSearchCV` performs two primary functions:\n",
    "\n",
    "* Cross-validation, to **identify bias and variance** during training\n",
    "* Grid Search, to **reduce bias and variance** by testing many combinations of model hyperparameters at once **without looking at our test data!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
