{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run __init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run src/load_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda install tqdm --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm \n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_train_df = data['adult']['train']['engineered']\n",
    "adult_train_target = data['adult']['train']['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_function_call(function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time()\n",
    "        result = function(*args, **kwargs)\n",
    "        execution_time = time() - start\n",
    "        return result, execution_time\n",
    "    return wrapper\n",
    "\n",
    "@time_function_call\n",
    "def model_fit(model, X, y):\n",
    "    return model.fit(X, y)\n",
    "\n",
    "@time_function_call\n",
    "def model_predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def run_model(model, model_name, data, labels):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels, stratify=labels)\n",
    "    \n",
    "    _, fit_time = model_fit(model, X_train, y_train)\n",
    "    \n",
    "    train_pred, train_pred_time = model_predict(model, X_train)\n",
    "    \n",
    "    test_pred, test_pred_time = model_predict(model, X_test)   \n",
    "    \n",
    "    return {\n",
    "            'model' : model,\n",
    "            'model_name' : model_name,\n",
    "            'f1_train_score' : f1_score(y_train, train_pred),\n",
    "            'f1_test_score' : f1_score(y_test, test_pred),\n",
    "            'accuracy_train_score' : model.score(X_train, y_train),\n",
    "            'accuracy_test_score' : model.score(X_test, y_test),\n",
    "            'fit_time' : fit_time,\n",
    "            'train_pred_time' : train_pred_time,\n",
    "            'test_pred_time' : test_pred_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F$_1$ Score by Penalty Type and C Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-4,4,9)\n",
    "for regularization_type in ['l1', 'l2']:\n",
    "    for c in tqdm(c_values):\n",
    "        model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "        test_result = run_model(model, \n",
    "                                regularization_type, \n",
    "                                adult_train_df, \n",
    "                                adult_train_target)\n",
    "        test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "for regularization_type in ['l1', 'l2']:\n",
    "    for c in tqdm(c_values):\n",
    "        model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "        test_result = run_model(model, \n",
    "                                regularization_type, \n",
    "                                adult_train_df, \n",
    "                                adult_train_target)\n",
    "        test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "for regularization_type in ['l1', 'l2']:\n",
    "    for c in tqdm(c_values):\n",
    "        model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "        test_result = run_model(model, \n",
    "                                regularization_type, \n",
    "                                adult_train_df, \n",
    "                                adult_train_target)\n",
    "        test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']\n",
    "plt.plot(l1_test_results.C, l1_test_results.f1_test_score, label='l1 penalty')\n",
    "plt.plot(l2_test_results.C, l2_test_results.f1_test_score, label='l2 penalty')\n",
    "plt.xscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = []\n",
    "c_values = np.logspace(-1,5,12)\n",
    "\n",
    "for _ in range(10):\n",
    "    for regularization_type in ['l1', 'l2']:\n",
    "        for c in tqdm(c_values):\n",
    "            model = LogisticRegression(penalty=regularization_type, C=c)\n",
    "            test_result = run_model(model, \n",
    "                                    regularization_type, \n",
    "                                    adult_train_df, \n",
    "                                    adult_train_target)\n",
    "            test_results.append(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(test_results)\n",
    "test_results_df['C'] = test_results_df.model.apply(lambda model: model.C)\n",
    "l1_test_results = test_results_df[test_results_df.model_name == 'l1']\n",
    "l2_test_results = test_results_df[test_results_df.model_name == 'l2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.cm import viridis_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = int(len(viridis_r.colors)/12)\n",
    "colors = [col for i, col in enumerate(viridis_r.colors) if i % 21 == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2,1,figsize=(20,12))\n",
    "for i, C in enumerate(l1_test_results.C.unique()): \n",
    "    l1_test_results_for_C = l1_test_results[l1_test_results.C == C]\n",
    "    sns.distplot(l1_test_results_for_C.f1_test_score, label=str(C), ax=ax1, color=colors[i])\n",
    "    l2_test_results_for_C = l2_test_results[l2_test_results.C == C]\n",
    "    sns.distplot(l2_test_results_for_C.f1_test_score, label=str(C), ax=ax2, color=colors[i])\n",
    "ax1.set_xlim(0.6,0.7)\n",
    "ax1.set_title('Distribution of L1 penalty performance')\n",
    "ax1.legend()\n",
    "ax2.set_xlim(0.6,0.7)\n",
    "ax2.set_title('Distribution of L2 penalty performance')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
